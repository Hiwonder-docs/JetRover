# 13 ROS1-Voice Control Lesson

## 13.1 **Voice Control Basic Lesson**

This document serves as an informational guide, providing an overview of the 6-channel circular microphone array, including its introduction, assembly, and debugging processes. It is specifically designed for single-module use.

### 13.1.1 R818 Noise Reduction Board

* **R818 Noise Reduction Board Introduction**

The R818 noise reduction board serves as a voice front-end solution featuring a multi-microphone array. Powered by a high-performance quad-core edge computing processor, this module internally incorporates iFLYTEK's voice algorithm. Leveraging the spatial filtering characteristics of the microphone array, it uses angle positioning to awaken individuals. This creates a directional pickup beamforming, suppressing noise outside the beam and enhancing far-field audio pickup quality. Specifically designed for human-machine interaction terminals, it integrates a high-performance echo cancellation algorithm to alleviate the challenges of speech and semantic recognition. Developers can seamlessly integrate this module to empower products with functionalities like multi-microphone audio capture, wake-up, noise reduction, and echo cancellation.

* **R818 Noise Reduction Board Specification**

The purpose of the interface on the R818 noise reduction board is outlined below:

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image3.png" style="width:500px"  />

| **Interface NO.** |          **Name**          |                    **Function**                    |
| :---------------: | :------------------------: | :------------------------------------------------: |
|       **1**       |      **Serial port**       |           For PC software communication            |
|       **2**       | **Reference signal port**  | Power amplifier/echo cancellation reference signal |
|       **3**       |    **Microphone port**     |       Connect to 6-channel microphone array        |
|       **4**       | **Independent power port** |                  Power input port                  |
|       **5**       |        **UAC port**        |                 Audio output port                  |

* **R818 Noise Reduction Board Parameter Description**

**1. Performance Parameter Indicator**

Key parameter:

|      **Microphone**       |                        SPA1687LR5H-1                         |
| :-----------------------: | :----------------------------------------------------------: |
|      **Sensitivity**      |                           -3dBV/Pa                           |
| **Signal-to-noise ratio** |                             65dB                             |
|       **PCB size**        |                      90mm\* 50mm\*1.2mm                      |
|     **External port**     | Serial port, UAC port, independent power port, reference signal port and microphone port |

Electrical parameter:

<table border="1" cellpadding="6" cellspacing="0" style="text-align: center; width: 100%; border-collapse: collapse;">
  <tr>
    <td style="font-weight: bold;" colspan="2">Project</td>
    <td style="font-weight: bold;">Minimum value</td>
    <td style="font-weight: bold;">Classic value</td>
    <td style="font-weight: bold;">Maximum value</td>
  </tr>
  <tr>
    <td style="font-weight: bold;">Working voltage</td>
    <td>DC5V</td>
    <td>4.75V</td>
    <td>5V</td>
    <td>5.25V</td>
  </tr>
  <tr>
    <td style="font-weight: bold;">Working current</td>
    <td>DC5V</td>
    <td>250mA</td>
    <td>300mA</td>
    <td>400mA</td>
  </tr>
  <tr>
    <td style="font-weight: bold; vertical-align: middle;" rowspan="2">Operating environment</td>
    <td>temperature</td>
    <td>-20℃</td>
    <td>25℃</td>
    <td>70℃</td>
  </tr>
  <tr>
    <td>relative humidness</td>
    <td>/</td>
    <td>/</td>
    <td>95%</td>
  </tr>
</table>
**2. Structure & Size**

Front view:

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image4.png" style="width:500px"  />

Back view:

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image5.png" style="width:500px"  />

### 13.1.2 6-Channel Circular Microphone Array

* **6-Channel Circular Microphone Array Introduction**

The 6-Channel Circular Microphone Array is a board designed for microphone pickup, known for its heightened sensitivity and excellent signal-to-noise ratio. It incorporates six analog silicon microphones arranged in a circular pattern. When integrated with the mainboard, it excels in delivering advanced features such as Acoustic Echo Cancellation (AEC), reduction of environmental noise, and factory-level sound pickup capabilities extending up to 10 meters.

* **6-Channel Circular Microphone Array Specification**

The structure of the circular 6-channel microphone array is as below:

Front： Back：

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image6.png" style="width:500px"  />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image7.png" style="width:500px"  />

1. Signal interface: connects to R818 noise reduction board.

2. Microphone Positions: Six analog silicon microphones are arranged in a circular pattern.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image8.png" style="width:500px"  />

* **6-Channel Circular Microphone Array Parameter**

**1. Performance Parameter**

Key parameter:

|      **Microphone**       | SPA1687LR5H-1                      |
| :-----------------------: | ---------------------------------- |
|      **Sensitivity**      | -3dBV/Pa                           |
| **Signal-to-noise ratio** | 65dB                               |
|       **PCB size**        | 115mm\* 12mm\*1.2mm                |
|     **External port**     | Microphone Board Signal Interfaces |

Electrical parameter:

<table border="1" cellpadding="6" cellspacing="0" style="text-align: center; width: 100%; border-collapse: collapse;">
  <tr>
    <td style="font-weight: bold;" colspan="2">Parameter</td>
    <td style="font-weight: bold;">Minimum value</td>
    <td style="font-weight: bold;">Classic value</td>
    <td style="font-weight: bold;">Maximum value</td>
  </tr>
  <tr>
    <td style="font-weight: bold;">Working voltage</td>
    <td>MICBIAS</td>
    <td>/</td>
    <td>3.3V</td>
    <td>/</td>
  </tr>
  <tr>
    <td style="font-weight: bold;">Working current</td>
    <td>MICBIAS</td>
    <td>/</td>
    <td>0.8mA</td>
    <td>10mA</td>
  </tr>
  <tr>
    <td style="font-weight: bold; vertical-align: middle;" rowspan="2">Operating environment</td>
    <td>Temperature</td>
    <td>-20℃</td>
    <td>25℃</td>
    <td>70℃</td>
  </tr>
  <tr>
    <td>Relative humidness</td>
    <td>/</td>
    <td>/</td>
    <td>95%</td>
  </tr>
</table>
**2. Product Structure & Size**

Front view:

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image9.png" style="width:500px"  />

Back view:

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image10.png" style="width:500px"  />

### 13.1.3 Wiring & Serial Port Debugging

* **6-Channel Circular Microphone Array Wiring**

1) Required Materials: R818 Noise Reduction Board, Circular Six-Microphone Array, Microphone Connection Cable, Serial Connection Cable, UAC Cable, Type-C Data Cable, Circular Microphone Array Outer Shell, Round Cross-Head Screws, 2-in-1 Adapter Board.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image11.png" style="width:500px"  />

2. Connect the microphone array to the noise reduction board. Be cautious while inserting the connection cable to avoid any forceful insertion that may lead to damage to the port.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image12.png" style="width:500px"  />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image13.jpeg" style="width:500px"  />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image14.png" style="width:500px"  />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image15.png" style="width:500px"  />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image16.png" style="width:500px"  />

3. Connect the UAC cable and Type C cable to the ports of the 2-in-1 adapter as shown in the picture.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image17.png" style="width:500px" />

4. Connect the serial port connection cable and UAC cable to the corresponding port on the noise reduction board.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image18.png" style="width:500px"  />

5. Install the circular microphone array onto the outer shell and secure it with screws. While installing, please take note of the signal port's location, highlighted within a yellow box.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image19.png" style="width:500px"  />

6. Proceed by attaching the pre-wired noise reduction board to the designated slots on the outer shell. Ensure the correct orientation of the noise reduction board, as depicted in the diagram below. Utilize 3M adhesive to firmly attach the Two-in-One Adapter over the neatly organized signal cables. Finally, adhere it to the isolation board of the noise reduction board, following the guidance of the green box in the diagram below.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image20.png" style="width:500px"  />

7. Plug in the Type-C cable to the computer's USB port for testing. Check for a lit indicator light in the red-boxed area below to ensure a proper power connection. This indicates the successful completion of the wiring and installation for the 6-channel circular microphone array.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image21.png" style="width:500px"  />

* **Install the Serial Port Driver**

1)  Locate the compressed package within the **"Serial Port Debugging Tool"** folder provided in the same directory as this document and extract it.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image22.png" style="width:500px"  />

2. Double-click the executable file **"SerialPortDebugTool.exe"** to launch the serial port debugging tool.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image23.png" style="width:500px"  />

3. The interface can be divided into four main areas: the Display Area, Serial Port Selection Area, Command Input Area, and Button Area, as illustrated in the figure below.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image24.png" style="width:500px"  />

1.  Display Area: Shows current information from the serial port and the microphone array.

2.  Serial Port Selection Area: Select the port number corresponding to the connected microphone array.

3.  Command Input Area: Enter commands for controlling the microphone array.

4.  Button Area: Provides controls for the microphone array. The specific functions of each button are detailed in the table below.

* **Debug the Circular Six-Microphone Array**

> [!NOTE]
>
> **Note: During recording, the microphone array enhances sounds arriving from the main-microphone direction while attenuating those from other directions, thereby improving captured audio quality and clarity.**

1. In the Serial Port Selection Area, select the port assigned to the microphone array;this guide uses COM3 as an example. (**Note: The port number is not fixed—never select COM1, which is reserved for system use**.)

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image25.jpeg" style="width:500px"  />

   > [!NOTE]
   >
   > **Note: If several devices are connected and you are unsure which port belongs to the microphone, right-click “This PC,” select “Manage,” open Device Manager, and look for an entry containing “CH340”—the corresponding COM number is the microphone’s port.**

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image26.png" style="width:500px"  />

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image27.png" style="width:500px"  />

2. Click **"Open Port"** in the serial port selection area. A page as shown below indicates a successful connection has been established with the PC.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image28.jpeg" style="width:500px"  />

3. Enter the command **{"type":"version"}** in the Command Input Area. Click **"Send Raw"** to view the microphone array's version information in the Display Area.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image29.png" style="width:500px"  />

4. Enter the command **{"type":"wakeup_keywords", "content":{"keyword": "Hello Hiwonder", "threshold": "900"}}** in the Command Input Area. Click **"Send Raw"** to view the communication status information from the microphone array in the Display Area. (**"content"**: wake up field, **"keyword"**: wake-up phrase; You can change it as you want.)

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image30.png" style="width:500px"  />

5. Now say **"Hello Hiwonder"** toward the microphone. When the wake-up phrase is detected, the Display area will print the ring angle and the recognized keyword.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image31.png" style="width:500px"  />

   The **"angle"** parameter indicates the azimuth angle.

   The **"keyword"** parameter indicates the specific wake-up phrase.

* **Use the Six-Microphone Array on a PC**

1. Open the recording application on the PC. Click **"Start Recording"**.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image32.png" style="width:500px"  />

2. After the recording is complete, click **"Play"** to listen to the audio.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image33.png" style="width:500px"  />

### 13.1.4 Virtual Machine Installation and Configuration

* **Install VMware Workstation Software**

A virtual machine is essentially a computer generated through software emulation, offering a usage experience similar to that of a physical computer. Popular virtual machine software options include VMware Workstation (often called VMware), VirtualBox, Microsoft Virtual PC, and more. In this section, we'll use the example of installing VMware Workstation, initiating, and configuring a virtual machine for explanation.

1. Extract the virtual machine software package saved in ‘**[2. Software Tool\\ 6. Voice Environment Setup Tool\\ Virtual Machine Installation]()**’.

2. Find the folder where the virtual machine was extracted, and double-click on the virtual machine executable file.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image34.png" style="width:500px"  />

3. Follow the images below to complete the virtual machine installation.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image35.png" style="width:500px" />

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image36.png" style="width:500px" />

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image37.png" style="width:500px" />

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image38.png" style="width:500px" />

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image39.png" style="width:500px" />

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image40.png" style="width:500px" />

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image41.png" style="width:500px" />

* **Start Virtual Machine**

1. Enter the virtual machine interface, then click-on ‘**Open a Virtual Machine**’.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image42.png" style="width:500px" />

2. Open the following image on the pop-up window. The image file is stored in “**[2. Software Tool-\>6. Voice Control Environment Setup Tool-\>Circular 6-Channel Microphone Array Image]()**. (The image name is subject to the actual item. This image is for reference only.)

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image43.png" style="width:500px" />

3. Rename this virtual machine and change the storage path. For example, name it hiwonder and store it in D: Drive. After setting, click **“Import”.**

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image44.png" style="width:500px" />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image45.png" style="width:500px" />

4. After importing, power on this virtual machine.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image46.png" style="width:500px" />

5. The virtual machine interface is as pictured.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image47.jpeg" style="width:500px"  />

* **Change Source**

> [!NOTE]
>
> **NOTE: this step matters, and please do not skip. Otherwise, installation package downloading will go wrong due to the internet.**

Source functions as APP Store on iOS and Android, where users can download and update.

Ubuntu’s default software download server is officially designated. If the files is downloaded slowly or error occurs during downloading, you can switch the software server to other source for downloading.

Take changing into aliyun for example. And you can select the appropriate source based on your country or region.

1. Click the buttons as pictured to check the network status.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image48.jpeg" style="width:500px"  />

2. Then search “**Software & Updates**”, and double click the icon to open.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image49.jpeg" style="width:500px"  />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image50.png" style="width:500px"  />

3)  Click “**Download from-\>Other**”.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image51.png" style="width:500px"  />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image52.png" style="width:500px"  />

4)  Scroll the list to find aliyun server. (China-\>mirrors.aliyun.com)

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image53.png" style="width:500px"  />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image54.png" style="width:500px"  />

5)  If you are not sure which server is suitable, you can click “**Select Best Server**”.

After selection, you are required to input the set password.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image55.jpeg" style="width:500px"  />

6)  Finishing authentication, you can click “**close**”.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image56.jpeg" style="width:500px"  />

7. If the following dialog box pops up, just click “**Rwload**” to update. Then close the page after update.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image57.jpeg" style="width:500px"  />

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image58.jpeg" style="width:500px"  />

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image59.jpeg" style="width:500px"  />

* **Initial Configuration**

The VM image ships with an English desktop; all later course work is done in English. To use menus and input in a different language (**this example demonstrates switching to Chinese**), change the locale setting as follows: Detailed steps are as follows:

1)  Click the button in the lower-left corner of the system, type **"Language Support"** in the search bar, and double click to open it.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image49.jpeg" style="width:500px"  />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image60.jpeg" style="width:500px"  />

2)  In the opened window, click **"Install"** to download the language pack.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image61.png" style="width:500px" />

3. Enter the password, **"ubuntu,"** and click **"Authenticate"**.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image55.jpeg" style="width:500px"  />

4. Wait for the installation to complete.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image62.png" style="width:500px" />

5)  Once downloaded, click the **"Install/Remove Language"** button. Select **"Chinese (simplified)"** and click **"Apply"**.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image63.png" style="width:500px" />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image64.jpeg" style="width:500px"  />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image65.png" style="width:500px" />

6)  After installation, return to the main interface and drag **"Chinese (simplified)"** from the last position to the top of the list.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image66.png" style="width:500px" />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image67.png" style="width:500px" />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image68.png" style="width:500px" />

7)  Switch to the **"Regional Formats"** tab, select **"Chinese (simplified)"**, and click **"Apply System-Wide"**. If prompted for password confirmation, enter the password to authorize.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image69.png" style="width:500px" />

8)  Enter the password **"ubuntu"** again, and after authentication, click **"Close"**.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image55.jpeg" style="width:500px"  />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image70.jpeg" style="width:500px"  />

9)  Configure the software as shown in the image below.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image71.png" style="width:500px" />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image72.png" style="width:500px" />

10) After restarting, you will see that the interface has been successfully set to Chinese. Click **"Update Names"** to confirm.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image73.png" style="width:500px" />

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image74.jpeg" style="width:500px"  />

**5. Change Resolution Ratio**

If you need to change the resolution, follow the instructions to operate.

1. Open “**Settings**”, and click search button at the bottom left corner, then input “**display**”.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image75.png" style="width:500px" />

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image76.png" style="width:500px" />

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image77.png" style="width:500px" />

2. Select the resolution you want, then follow the pop-up instructions to complete modification.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image78.png" style="width:500px" />

3. The final display effect is as follow.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image79.png" style="width:500px" />

### 13.1.5 Replace Voice Resource Package and APPID

* **Apply for Offline Voice Resources and ID**

Since this system uses offline speech recognition, you need to apply for the offline resource package from iFLYTEK.

1. Open the iFLYTEK Open Platform webpage at:

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image80.png" style="width:500px"  />

2) Click on **"Console"** -\> **"My Applications"** in the upper right corner of the webpage and create a new application.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image81.png" style="width:500px"  />

3) Fill in the required fields and click Submit.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image82.png" style="width:500px"  />

4. Open the newly created application.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image83.png" style="width:500px" />

5. Navigate to **"Speech Recognition"** **-\>** **"Offline Command Word Recognition"**. Locate the corresponding APPID within the red box area shown in the figure below. Then, find **"Offline Command Word Recognition SDK"** **-\>** **"Linux MSC"** and click to download.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image84.png" style="width:500px"  />

   > [!NOTE]
   >
   > **Note: Each app is valid for 90 days for free. After expiration, a new application must be created. Each user can create up to five applications. The process for creating a new app is the same.**

* **Replace Offline Voice Resources and ID**

1. Unzip the file below and locate the offline voice resource **'common.jet'.**

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image85.png" style="width:500px"  />

2. Copy and paste the file to the virtual machine desktop.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image86.png" style="width:500px" />

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image87.png" style="width:500px" />

3. Open the folder containing the off--line voice resources located in “**ros_ws/src/xf_mic_asr_offline/config/msc/res/asr/**”, then delete files shown in red frame.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image88.png" style="width:500px" />

4. Put “**common.jet**” file you just copied into this folder.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image89.png" style="width:500px" />

5. Move to the folder stored in “**ros_ws/src/xf_mic_asr_offline/launch**”. Right click “**mic_init.launch**” file, and select “**Open with Text Editor**”.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image90.png" style="width:500px" />

6. Replace the existing appid with the one obtained from the iFLYTEC website. Afterward, press **'Ctrl+S'** to save the changes. (Note: the offline voice resource should be consistent with the appid)

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image91.png" style="width:500px" />

* **Change User Parameters**

After modifying the APPID, if you wish to make changes to the speech recognition threshold, recording duration, wake-up word, or wake-up speech, you can refer to the methods described below.

1. Locate the folder ‘**ros_ws/src/xf_mic_asr_offline/launch**’, and right-click ‘**mic_init.launch**’, then select ‘**Open With Text Editor**’.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image92.png" style="width:500px" />

2. Set confidence thresholds for speech results and the duration of each voice command entry based on the two parameters highlighted in the red box in the following diagram.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image93.png" style="width:500px"  />

3. Modify the parameter highlighted in the red box below to change the wake word.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image94.png" style="width:500px"  />

   English wake word format: word1 word2 word3

4. To change the wake language, you can directly change the below parameter.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image95.png" style="width:500px"  />

5. After modification, press **‘Ctrl+S’** to save the editing.

### 13.1.6 Configure Microphone Port

* **Configuration Introduction**

We now need to configure the serial port in preparation for initializing the microphone node in subsequent lessons.

* **Configuration Steps**

1) Start the virtual machine.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image47.jpeg" style="width:500px"  />

2. Please refer to **"[13.1.5 Replacing the Voice Resource Package and APPID / Applying for Offline Voice Resources and APPID]()"** located in the same path as this document to apply for and replace the offline voice resource package.

3. Connect the microphone array to the computer according to section ‘**[13.1.3 Wiring & Serial Port Debugging/ 6-Channel Circular Microphone Array Wiring]()**’.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image96.png" style="width:500px" />

4. Connect the device to the virtual machine.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image97.png" style="width:500px"  />

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image98.png" style="width:500px"  />

5. After successful connection, press the short-cut ‘**Ctrl+Alt+T**’ to open the command-line terminal. Then execute the command ‘**ls /dev/tty\***’ to check the serial port number of the microphone array, as highlighted in the red box.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image99.png" style="width:500px" />

6. Enter the command to navigate to the file directory, then enter the command to open the launch file.

   ```py
   cd ros_ws/src/xf_mic_asr_offline/launch/
   ```

   ```py
   vim mic_init.launch
   ```


7. Change the serial port number within the red box to **"/dev/ ttyCH341USB0"** obtained in step 5, then save and exit.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image101.png" style="width:500px" />

8. Enter the following command, press Enter, and input your password to open the rules file.

   ```py
   sudo vim /etc/udev/rules.d/xf_mic.rules
   ```

9. Enter the code.

   “**ATTRS{idVendor}=="1a86", ATTRS{idProduct}=="7523", MODE="0666"**” Grant access to the microphone serial port, save and exit.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image103.png" style="width:500px" />

### 13.1.7 Wake Up Microphone Through Command

This section explains how to start the microphone node via command and print the wake-up angle to the terminal after successful voice activation.

* **Operation Steps**

1. Enter the following command to start the six-microphone array initialization node.

   ```py
   roslaunch xf_mic_asr_offline mic_init.launch
   ```

2. After initialization is complete, the terminal will display content as shown in the figure below. The first startup may be slightly slower. The successful startup is shown in the figure:

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image105.png" style="width:500px"  />

3. Then, speak the wake word **"Hello Hiwonder"** towards the microphone to activate it. After being awakened, the terminal will print its wake-up angle, as shown in the following figure:

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image106.png" style="width:500px"  />

4. To shut down this node, press "Ctrl+C" in the terminal window.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image107.png" style="width:500px"  />

5. Enter the command to start the microphone node:

   ```py
   roslaunch xf_mic_asr_offline mic_init.launch
   ```

6. The default wake up word is set to **"Hello Hiwonder"**. Simply say **"Hello Hiwonder"** towards the six-microphone array to begin interaction.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image106.png" style="width:500px"  />

   After wake-up, the microphone's wake-up angle is 259, indicating the direction of the sound source relative to the circular six-microphone array. (For content related to voice recognition, please proceed to **"[13.3 Voice Interaction Application]()"** for further study.)

   > [!NOTE]
   >
   > **Note: If you said "Hello Hiwonder" but the six-microphone array did not respond, you need to re-enter the command "start the six-microphone array ROS node". (The microphone does not need to be re-plugged).**

* **Change Wake Word**

1. Navigate to the folder ‘**ros_ws/src/xf_mic_asr_offline/launch**’, and right-click the file **mic_init.launch,** then select ‘**Open With Text Editor**’.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image92.png" style="width:500px" />

2. Locate the following code.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image94.png" style="width:500px"  />

   English wake-up word syntax: "Word1 Word2 Word3 ..."

3. Take revising Chinese wake word as example. Change ‘**xiao3 huan4 xiao3 huan4**’ to ‘**ping2 guo3 ping2 guo3**’.

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image109.png" style="width:500px"  />

4. Use short-cut **‘Ctrl+S’** to save the editing.

5. Execute the command to initiate voice wake up function.

   ```py
   roslaunch xf_mic_asr_offline mic_init.launch
   ```

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image110.png" style="width:500px"  />

   > [!NOTE]
   >
   > **Note: If you said "ping2 guo3 ping2 guo3**’**" but the six-microphone array did not respond, you need to re-enter the command "start the six-microphone array ROS node". (No need to unplug or reconnect the microphone.)**

* **FAQ**

**1. 10108 Error Code**

Repeatedly starting the six-microphone array initialization service may occasionally result in the issue shown in the figure below:

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image111.png" style="width:500px" />

This issue does not affect normal operation. Simply speak the wake word **"Hello Hiwonder"** again, and the system will return to its normal state.

<img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image106.png" style="width:500px"  />

**2. Solution for Unable to Find the Microphone Device**

> [!NOTE]
>
> **Note: If you encounter the prompt "Cannot find the microphone device," follow the steps below to install the microphone driver.**

1. Execute the command to switch to the catalog containing the driver programs.

   ```py
   cd ros_ws/src/CH341SER_LINUX/driver/
   ```

2. Run the command ‘**make**’ to start compilation.

   ```py
   make
   ```

3. Execute the command to load the driver program.

   ```py
   sudo make load
   ```

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image114.png" style="width:500px" />

4. Run the command to install the program to the system.

   ```py
   sudo make install
   ```

   <img src="../_static/media/3/section_123_Voice Control Basic Lesson(Circular Microphone Array)/media/image115.png" style="width:500px" />

5. After the driver installation is complete, re-enter the command to start the microphone node.

   ```py
   roslaunch xf_mic_asr_offline mic_init.launch
   ```

## 13.2 Six-Microphone Circular Array Installation

1)  Secure the six-microphone circular array using four M4×6 round-head Phillips screws.

<img src="../_static/media/3/section_124_1 Six-Microphone Circular Array Installation/media/image2.png" style="width:500px"  />

2)  The image below shows the assembled six-microphone array.

<img src="../_static/media/3/section_124_1 Six-Microphone Circular Array Installation/media/image3.png" style="width:500px"  />

3)  Connect the microphone array to Port 10 of the USB hub located at the bottom of the robot using a USB cable.

<img src="../_static/media/3/section_124_1 Six-Microphone Circular Array Installation/media/image4.png" style="width:500px" />

## 13.3 **Voice Interaction Application**

### 13.3.1 6-Channel Microphone Array Configuration (Must Read)

* **Apply for Offline Voice Resources and ID**

As the robot utilizes offline voice recognition, you will require offline voice resources available exclusively on the iFLYTEK Chinese website. Please note that you need to switch the language to Chinese and follow the provided instructions for the process.

1. Enter iFLYTEK website, **https://www.xfyun.cn/,** then click “**sign in**” to create an account.

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image3.png" style="width:500px"  />

2. Choose “**Sign up with phone number**”, and fill in the required information. (select corresponding country code)

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image4.png" style="width:500px"  />

3. After registration, click **“console-\>my application”** to add new application.

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image5.png" style="width:500px"  />

4)  Fill in the required information, and click “**Submit**”.

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image6.png" style="width:500px"  />

5. Open the application you just added.

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image7.png" style="width:500px" />

6. Click “**offline voice command recognition**”. Then find “**APPID**” in the drop-down menu as the red frame shown. Next, find “**offline voice command recognition SDK-\> Liunx MSC**”, and click “**Download**”.

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image8.png" style="width:500px"  />

7. Select "**Linux**", check the desired functions, click "**SDK Download**"

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image9.png" style="width:500px"  />

8. Click to return to the old version, then click "**OK**" to start the download.

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image10.png" style="width:500px" />

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image11.png" style="width:400px"  />

   9. You can click **"Personalized Wake Word Experience Package"**, set the wake word as "**Xiao Mai Xiao Mai**", and then click "**Submit**".

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image12.png" style="width:500px" />

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image13.png" style="width:500px" />

   10. Click "**Go to SDK Download Center**", and then repeat step 7 to start the download.

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image14.png" style="width:500px" />

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image15.png" style="width:500px"  />

   > [!NOTE]
   >
   > **Note: Each new application offers 90-day free trial, and you will be charged if you continue to use it. When free trial expires, you can add new application again, and each person can only request 5 new applications.**

* **Replace Offline Voice Resources and ID**

1. Extract the downloaded compressed file of the voice SDK.

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image16.png" style="width:500px" />

   > [!NOTE]
   >
   > **Note: The file is downloaded to the path you have set.**

2. Open the extracted folder “**Linux_aitalk_exp1227_216da28f**” **(The version ID** 1227_216da28f **is not a fixed version and is distributed by the official source**). Click “**\bin\msc\res\asr**” to find “**common.jet**”, then drag this file to the system image desktop.

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image17.png" style="width:500px" />

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image18.png" style="width:500px" />

3. Open the Ubuntu system within the virtual machine, and navigate to the following path:

   **"Home/ros_ws/src/xf_mic_asr_offline/config/msc/res/asr/"**, enter the folder where the offline speech resources are stored, and delete the files marked in red.

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image19.png" style="width:500px" />

4. Put “**common.jet**” file you just copied into this folder.

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image20.png" style="width:500px" />

5. Move to the folder stored in “**Home/ros_ws/src/xf_mic_asr_offline/**”. Right click “**mic_init.launch**” file, and select “**Open with Text Editor**”.

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image21.png" style="width:500px" />

6. Replace the existing appid with the one obtained from the iFLYTEK website. Afterward, press **'Ctrl+S**' to save the changes.

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image22.png" style="width:500px" />

7. After modifying the APPID and replacing the common.jet file, connect to the robot's remote desktop and double-click <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image23.png" style="width:50px" /> to open it. Locate the 'Voice Function' option and set it to 'English' to enable English language support for subsequent voice interactions.

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image24.png" style="width:500px" />

### 13.3.2 **Voice-Controlled Car Movement**

This lesson focuses on issuing voice commands to control the robot's movements. For example, you can use voice commands to make the robot move forward, backward, and perform other corresponding actions.

* **Overview**

<span class="mark">Firstly, subscribe to the speech recognition service published by the microphone array node. Perform positioning, noise reduction, and recognition on the speech signal to obtain the recognized sentence and the angle of the sound source. Activate the chassis movement control service of the robot to enable direct execution of voice commands.</span>

Then, use a specific wake word to wake u<span class="mark">p the robot, which then provides corresponding voice reply.</span>

Finally, perform sentence matching. Based on the matching result, the robot will execute the corresponding action.

* **Preparation**

1)  Please ensure the microphone array, sound card and speaker are connected to the corresponding port of USB hub.

2)  Please refer to the "[**13.3.1 (Must-Read) Six-Microphone Array Installation**]()" section in this catalogue to complete the application of APPID and the replacement of files for the Jetson Nano part.

3)  The default system wake word is "hello hiwonder" in English. To switch to Chinese, refer to the documentation under the directory of **[13.4 Switching Wake Words Between Chinese and English]()**

* **Operation Steps**

> [!NOTE]
>
> **Notice: The input command should be case sensitive, and keywords can complemented using Tab key.**

1. Start the robot, and access the robot system desktop using remote control software NoMachine. For connection instructions, please refer to: **[1. Quick Start Guide-\> 1.6 Development Environment Setup and Configuration]()**

2. Click-on <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image25.png" style="width:50px" /> to open the command-line terminal.

3. Run the command and hit Enter to disable the app auto-start service.

   ```py
   sudo systemctl stop start_app_node.service
   ```

4. Execute the command and hit Enter to enable the voice control function.

   ```py
   roslaunch xf_mic_asr_offline voice_control_move.launch
   ```

5. If you need to terminate this program, use short-cut **“Ctrl+C”.** If the program fails to stop, please retry.

6. To revisit the game of the mobile app later, simply enter command to restart the related services, press **Enter** and wait for the robot arm to return to its initial posture.

   ```py
   sudo systemctl start start_app_node.service
   ```

* **Program Outcome**

> [!NOTE]
>
> **Note:**
>
> * **For a better experience, please ensure you are in a relatively quiet environment.**
>
> * **Before issuing each voice command, start by saying the wake word.**
>
> * **Speak loudly and clearly when giving voice commands.**
>
> * **Give voice commands one at a time and wait for the robot to complete one feedback before giving additional commands.**

1)  Once the game has started, wait until you hear the speaker say "**I’m ready**" before providing any voice input.

2)  To begin, say the wake word **"Hello Hiwonder"** and wait for the speaker to play **"I’m here"** before proceeding with the next voice command. For instance, say **"go forward"**; upon recognition of the voice command by the robot, the speaker will play **"Copy that. Start moving forward"**, and then the robot will execute the command accordingly.

The following are voice commands and their corresponding control actions:

| **Voice Command** |                  **Control Action**                   |
| :---------------: | :---------------------------------------------------: |
|    Go forward     |          Control the robot car to go forward          |
|    Go backward    |         Control the robot car to go backward          |
|     Turn left     |          Control the robot car to turn left           |
|    Turn right     |          Control the robot car to turn right          |
|     Come here     | Control the car to move to the front of the commander |

* **Program Analysis**

Voice-controlled robot involves establishing a connection between the voice control node and the underlying driver node of the robot. Subsequently, the car is controlled to execute corresponding actions based on the commands issued through voice input.

**1. launch File**

The launch file path is located at:

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/launch/voice_control_move.launch**

**(1) Initiate Several launch Files**

The `<include>` tag is utilized to incorporate other launch files. Upon starting this file, additional launch files will be initiated simultaneously. In this context, we primarily begin with the initialization of the microphone, chassis control, radar, and other necessary startup files.

```py
    <include file="$(find xf_mic_asr_offline)/launch/mic_init.launch"/>
    <include file="$(find hiwonder_peripherals)/launch/lidar.launch"/>
    <!--底盘驱动(chassis driver)-->
    <include file="$(find hiwonder_controller)/launch/hiwonder_controller.launch"/>
```

**(2) Initiate Node**

The `<node>` tag represents the node initiated within the file, with subsequent parameters such as `<name>`, `<pkg>`, `<type>`, `<output>`, etc.

`<name>` signifies the node's identifier.

`<pkg>` denotes the package containing the node's functionality.

`<type>` specifies the file executed by the node; in this case, it refers to the contents of "**voice_control_move.py**" and "**init_pose.py**" files.

`<output>` designates the destination for output, typically the screen.

```py
    <node name="init_pose" pkg="hiwonder_slam" type="init_pose.py" output="screen"/>
    <node pkg="xf_mic_asr_offline" type="voice_control_move.py" name="voice_control_move_node" output="screen"/>
```

**2. Python Program**

The source code of this program is saved in

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/scripts/voice_control_move.py**

```py
class VoiceControlNode:
    def __init__(self, name):
        rospy.init_node(name)

        self.angle = None
        self.words = None
        self.running = True
        self.haved_stop = False
        self.lidar_follow = False
        self.start_follow = False
        self.last_status = Twist()
        self.threshold = 3
        self.speed = 0.3
        self.stop_dist = 0.4
        self.count = 0
        self.scan_angle = math.radians(45)

        self.pid_yaw = pid.PID(1.6, 0, 0.16)
        self.pid_dist = pid.PID(1.7, 0, 0.16)
```

**(1) Subscribe to Service**

Subscribe to the service for retrieving speech recognition words and wake angles, then establish connections with the underlying driver nodes.

```py
        self.mecanum_pub = rospy.Publisher('/hiwonder_controller/cmd_vel', Twist, queue_size=1)
        self.lidar_sub = rospy.Subscriber('/scan', sensor_msg.LaserScan, self.lidar_callback)
        rospy.Subscriber('/asr_node/voice_words', String, self.words_callback)
        rospy.Subscriber('/awake_node/angle', Int32, self.angle_callback)
```

**(2) Wake Up Robot Car**

Upon successful matching of the wake word, the corresponding voice is played using the voice_play module.

```
    def words_callback(self, msg):
        self.words = json.dumps(msg.data, ensure_ascii=False)[1:-1]
        if self.language == 'Chinese':
            self.words = self.words.replace(' ', '')
        print('words:', self.words)
        if self.words is not None and self.words not in ['唤醒成功(wake-up-success)', '休眠(Sleep)', '失败5次(Fail-5-times)',
                                                         '失败10次(Fail-10-times']:
            pass
        elif self.words == '唤醒成功(wake-up-success)':
            self.play('awake')
```

**(3) Print Sound Source Angle**

Using the sound source angle identified by the microphone array, calculate and print the robot's steering angle.

```py
    def angle_callback(self, msg):
        self.angle = msg.data
        print('angle:', self.angle)
        self.start_follow = False
        self.mecanum_pub.publish(Twist())
```

**(4) Lidar Obstacle Avoidance**

Based on the scanning data from the LiDAR, obtain the distance between the left and right sides and compare them to determine the direction and angle of the robot's rotation.

```py
    def lidar_callback(self, lidar_data: sensor_msg.LaserScan):
        twist = Twist()
        if self.lidar_type != 'G4':
            max_index = int(math.radians(MAX_SCAN_ANGLE / 2.0) / lidar_data.angle_increment)
            left_ranges = lidar_data.ranges[:max_index]  # 左半边数据
            right_ranges = lidar_data.ranges[::-1][:max_index]  # 右半边数据
        elif self.lidar_type == 'G4':
            min_index = int(math.radians((360 - MAX_SCAN_ANGLE) / 2.0) / lidar_data.angle_increment)
            max_index = int(math.radians(180) / lidar_data.angle_increment)
            left_ranges = lidar_data.ranges[::-1][min_index:max_index][::-1]  # 左半边数据
            right_ranges = lidar_data.ranges[min_index:max_index][::-1]  # 右半边数据
        if self.start_follow:
            angle = self.scan_angle / 2
            angle_index = int(angle / lidar_data.angle_increment + 0.50)
            left_range, right_range = np.array(left_ranges[:angle_index]), np.array(right_ranges[:angle_index])

            ranges = np.append(right_range[::-1], left_range)
            nonzero = ranges.nonzero()
            dist = ranges[nonzero].min()
            min_index = list(ranges).index(dist)
            angle = -angle + lidar_data.angle_increment * min_index  # 计算最小值对应的角度
            if dist < self.threshold and abs(math.degrees(angle)) > 5:  # 控制左右
                self.pid_yaw.update(-angle)
                twist.angular.z = misc.set_range(self.pid_yaw.output, -self.speed * 6, self.speed * 6)
            else:
                self.pid_yaw.clear()

            if dist < self.threshold and abs(self.stop_dist - dist) > 0.02:
                self.pid_dist.update(self.stop_dist - dist)
                twist.linear.x = misc.set_range(self.pid_dist.output, -self.speed, self.speed)
```

**(5) Voice Control**

Match the recognized text and play the corresponding voice through the voice_play module. Then, based on the recognized content, set the corresponding linear or angular velocity. This allows the underlying driver node mecanum_pub to publish messages and control the robot's movement.

```py
                 if self.words == '前进' or self.words == 'go forward':
                    self.play('go')
                    self.time_stamp = rospy.get_time() + 2
                    twist.linear.x = 0.2
                elif self.words == '后退' or self.words == 'go backward':
                    self.play('back')
                    self.time_stamp = rospy.get_time() + 2
                    twist.linear.x = -0.2
                elif self.words == '左转' or self.words == 'turn left':
                    self.play('turn_left')
                    self.time_stamp = rospy.get_time() + 2
                    twist.angular.z = 0.8
                elif self.words == '右转' or self.words == 'turn right':
                    self.play('turn_right')
                    self.time_stamp = rospy.get_time() + 2
                    twist.angular.z = -0.8
                elif self.words == '过来' or self.words == 'come here':
                    self.play('come')
                    if 330 > self.angle > 150:
                        twist.angular.z = 1
                        self.angle = self.angle - 150
                    else:
                        twist.angular.z = -1
                        if self.angle <= 150:
                            self.angle = 150 - self.angle
                        else:
                            self.angle = 150 + 360 - self.angle
                    self.time_stamp = rospy.get_time() + math.radians(self.angle)
                    self.lidar_follow = True
                elif self.words == '休眠(Sleep)':
                    rospy.sleep(0.01)
```

* **Function Extension**

**1. Change wake Command**

The default wake command is ‘Hello Hiwonder’, which can be changed by revising the configuration file. For example, change it to ‘**小爱小爱**’.

> [!NOTE]
>
> **Notice: The input command should be case sensitive, and keywords can be complemented using Tab key.**

1. Start the robot, and access the robot system using the remote control software NoMachine.

2. Click-on <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image25.png" style="width:50px" /> to open the command-line terminal.

3. Run the command and hit Enter to navigate to the directory containing configuration file.

   ```py
   roscd xf_mic_asr_offline/launch/
   ```

4. Execute the command and hit Enter to access the configuration file.

   ```py
   vim mic_init.launch
   ```

5)  Locate the following code.

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image39.png" style="width:500px" />

6)  Press ‘I’ key to enter the editing mode, and change the vale of ‘**chinese_awake_words**’ to ‘**小爱小爱**’.

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image40.png" style="width:500px" />

7)  After modification, press ‘**Esc**’ key, and input ‘**:wq**’, then hit Enter to save and exit the file.

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image41.png" style="width:500px" />

8)  Restart the game according to the instructions provided in ‘[**13.3.2 Voice-Controlled Car Movement-> Operation Steps**]()’.

### 13.3.3 Voice-Control Robotic Arm

* **Overview**

In this section, the voice recognition function installed on the robot was combined with the visual robotic arm to control it to perform the corresponding actions.

In the program, subscribe to the microphone array’s speech recognition service. This lets the robot handle tasks like locating the sound source, reducing background noise, and recognizing speech. From there, it can get both the recognized text and the angle of the sound source.

Next, after successfully waking up the robot and speaking specific commands, the robot will provide corresponding voice reply. Simultaneously, upon identifying specific colors, the robotic arm will be controlled to perform corresponding actions based on the voice commands.

You can complete this feature by following the “**Preparation**” section below, and then proceeding with the “**Operation Steps**” section for setup and usage.

* **Preparation**

1)  Before beginning, ensure that the voice module is properly installed on the robot, and connected to the USB ports of the USB hub.

2)  Please refer to the "[**13.3.1 Must-Read Six-Microphone Array Installation**]()" section in this catalogue to complete the application of APPID and the replacement of files for the Jetson Nano part.

3)  The default system wake word is "hello hiwonder" in English. To switch to Chinese, refer to the documentation under the directory of **[13.4 Switching Wake Words Between Chinese and English]()**

* **Operation Steps**

> [!NOTE]
>
> **Note: When entering commands, be sure to use correct case and spacing. You can use the Tab key to auto-complete keywords.**

1. Power on the robot and connect it via the NoMachine remote control software. Refer to: **[1. Quick Start Guide -\> 1.6 Development Environment Setup and Configuration]()**

2. Click the terminal icon <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image25.png" style="width:50px" /> in the system desktop to open a command-line window.

3. Enter the following command and press Enter to stop the APP service:

   ```py
   sudo systemctl stop start_app_node.service
   ```

4. Enter the command and press Enter to activate the voice-controlled robotic arm function for the vehicle:

   ```py
   roslaunch xf_mic_asr_offline voice_control_arm.launch
   ```

5. Once the program has loaded successfully, say the wake word “**hello hiwonder**”. When the speaker responds with “**I’m here**”, you may issue your command.

   Then, issue the voice command **pull a radish**. The robotic arm will grip the object directly in front of it. When the command **give it to me** is given, the arm will hand the gripped object to the user from the side.

> [!NOTE]
>
> **Note:**
>
> **For a better experience, please ensure you are in a relatively quiet environment.**
>
> **Before issuing each voice command, start by saying the wake word.**
>
> **Speak loudly and clearly when giving voice commands.**
>
> **Give voice commands one at a time and wait for the robot to complete one feedback before giving additional commands.**

6. To exit the feature, press **Ctrl+C** in the terminal. If the program does not close successfully, try pressing **Ctrl+C** again.

7. If you wish to re-experience the mobile app features later, you need to enter the command to restart the relevant service. After entering and executing the command, wait for the robotic arm to return to its initial posture.

   ```py
   sudo systemctl start start_app_node.service
   ```

* **Program Analysis**

Voice-controlled robotic arm involves establishing a connection between the voice control node and the underlying driver node of the robot. 
Subsequently, the robotic arm is controlled to execute corresponding actions based on the commands.

**1. Launch File**

The launch file path is located at:

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/launch/voice_control_arm.launch.py**

**(1) Initiate Several launch Files**

The `<include>` tag is utilized to incorporate other launch files. When this file is launched, it will simultaneously start all other launch files.

This primarily launches the microphone initialization and the servo control startup files.

```py
<include file="$(find xf_mic_asr_offline)/launch/mic_init.launch"/>
```

**(2) Initiate Node**

The `<node>` tag represents the node initiated within the file, with subsequent parameters such as`<name>`, `<pkg>`, `<type>`, `<output>`, etc.

`<name>` signifies the node's identifier.  

`<pkg>` denotes the package containing the node's functionality.

`<type>` specifies the file executed by the node; in this case, it refers to the contents of "**voice_control_move.py**" and "**init_pose.py**" files.

`<output>` designates the destination for output, typically the screen.

```
<node pkg="xf_mic_asr_offline" type="voice_control_arm.py" name="voice_control_arm_node" output="screen"/>
```

**2. Python Program**

The source code for this program is located at:

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/scripts/voice_control_arm.py**

```
    def __init__(self, name):
        rospy.init_node(name, anonymous=True)

        self.language = os.environ['ASR_LANGUAGE']
        rospy.Subscriber('/asr_node/voice_words', String, self.words_callback)
        self.controller = ActionGroupController(use_ros=True)
        rospy.sleep(1)
        self.controller.runAction('init')
        rospy.wait_for_service('/voice_control/get_offline_result')
        self.play('running')
        self.buzzer_pub = rospy.Publisher('/ros_robot_controller/set_buzzer', BuzzerState, queue_size=1)
        rospy.loginfo('唤醒口令: 小幻小幻(Wake up word: hello hiwonder)')
        rospy.loginfo('唤醒后15秒内可以不用再唤醒(No need to wake up within 15 seconds after waking up)')
        rospy.loginfo('控制指令: 拔个萝卜 拿给我(Voice command: pick a carrot/pass me please)')
```

**(1) Environment Variable Retrieval**

```
self.language = os.environ['ASR_LANGUAGE']
```

Obtain the configuration items `ASR_LANGUAGE` and `MIC_TYPE` from the system environment variables, representing the speech recognition language and microphone type respectively.

**(2) Subscriber Creation**

```
rospy.Subscriber('/asr_node/voice_words', String, self.words_callback)
```

Listen to the **`/asr_node/voice_words`** topic. When a message is received, call the **`self.words_callback`** method for processing.

**(3) Publisher Creation**

```
self.buzzer_pub = rospy.Publisher('/ros_robot_controller/set_buzzer', BuzzerState, queue_size=1)
```

Publish to the **/ros_robot_controller/set_buzzer** topic with the message type BuzzState.

**(4) Define callback method**

```
    def words_callback(self, msg):
        words = json.dumps(msg.data, ensure_ascii=False)[1:-1]
        if self.language == 'Chinese':
            words = words.replace(' ', '')
        print('words:', words)
        if words is not None and words not in ['唤醒成功(wake-up-success)', '休眠(Sleep)', '失败5次(Fail-5-times)', '失败10次(Fail-10-times']:
            if words == '拔个萝卜' or words == 'pick a carrot':
                self.play('ok')
                self.controller.runAction('voice_pick')
            elif words == '拿给我' or words == 'pass me please':
                self.play('come')
                self.controller.runAction('voice_give')
        elif words == '唤醒成功(wake-up-success)':
            self.play('awake')
        elif words == '休眠(Sleep)':
            msg = BuzzerState()
            msg.freq = 1900
            msg.on_time = 0.05
            msg.off_time = 0.01
            msg.repeat = 1
            self.buzzer_pub.publish(msg)
```

Execute corresponding action group upon keyword recognition.

**(5) Voice-Controlled Color Recognition**

This lesson focuses on activating or deactivating the color recognition function using voice commands. When any color—red, green, or blue—is recognized, the speaker will announce the corresponding color.

**(6) Overview**

First, the system subscribes to the voice recognition service published by the microphone array node. It processes the incoming voice data through localization, noise reduction, and recognition, and then retrieves the recognized sentence and the angle of the sound source.

Subsequently, use a specific wake word to wake up the robot. After saying the wake word, the robot will provide corresponding voice feedback and response.

Finally, publish the corresponding command. The robot will enable or disable color recognition based on the command.

The implementation of color recognition is divided into two parts: color detection and response execution.

In the recognition process, Gaussian filtering is first applied to reduce noise in the image. Then, the image is converted to the Lab color space to better distinguish colors.

Based on this, color thresholds are used to identify the color of the object within the circle. A mask is then applied to the image, which involves selecting parts of the image, graphics, or objects to globally or locally block out areas in the image for processing.

After masking, morphological operations including opening and closing are performed on the object image to refine the results.

* **Preparation**

1) Please ensure the microphone array, sound card and speaker are connected to the corresponding port of USB hub.

2) Please refer to the "[**13.3.1 Must-Read Six-Microphone Array Installation**]()" section in this catalogue to complete the application of APPID and the replacement of files for the Jetson Nano part.

3) The default system wake word is "hello hiwonder" in English. To switch to Chinese, refer to the documentation under the directory of **[13.4 Switching Wake Words Between Chinese and English]()**

* **Operation Steps**

> [!NOTE]
>
> **Notice:**
>
> * **The input command should be case sensitive, and keywords can be complemented using Tab key.**
>
> * **When identifying color blocks, ensure that objects with colors similar or identical to the color blocks in the background are avoided to prevent interference.**
>
> * **If color recognition is inaccurate, you can adjust the color threshold by referring to: 
>   [9. ROS1-ROS+OpenCV Course]()**

1. Start the robot, and access the robot system desktop using remote control software NoMachine. For connection instructions, please refer to: **[1. Quick Start Guide -\> 1.6 Development Environment Setup]()**

2. Click-on <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image25.png" style="width:50px" /> to open the command-line terminal.

3. Run the command and hit Enter to disable the app auto-start service.

   ```py
   sudo systemctl stop start_app_node.service
   ```

4. Run the command and hit Enter to initiate the game.

   ```py
   roslaunch xf_mic_asr_offline voice_control_color_detect.launch
   ```

5. To access the live camera feed, open a new terminal and execute the command '**rqt_image_view**' to activate the visualization tool. Then, select the color recognition and sorting channel '**/color_detect/image_result**'.

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image53.png" style="width:500px" />

6. If you need to terminate this program, use short-cut ‘**Ctrl+C**’. If the program fails to stop, please retry.

7. To revisit the game of the mobile app later, simply enter command to restart the related services, press Enter to execute the command, and wait for the robot arm to return to its initial posture.

   ```py
   sudo systemctl start start_app_node.service
   ```

* **Program Outcome**

> [!NOTE]
>
> **Notice:**
>
> * **For optimal performance, ensure you are in a relatively quiet environment.**
>
> * **Before issuing each voice command, start by saying the wake word.**
>
> * **Speak loudly and clearly when giving voice commands.**
>
> * **Give voice commands one at a time and wait for the robot to provide feedback before issuing additional commands.**

To initiate game, begin by stating the wake word "**Hello Hiwonder**," followed by the command **"Start color recognition"** to prompt the robot to start recognizing colors. The recognizable colors for this activity are red, green, and blue. Upon recognizing a color, such as red, position a red square within the camera's field of view. Once the color block is identified, the robot will announce the color name, such as "**red**."

<span class="mark">If you wish to cease color recognition, please state the wake word "Hello Hiwonder" followed by the command **"Stop color recognition".**</span>

* **Program Analysis**

Voice-controlled color recognition enables the voice control node to establish communication with the robot's underlying driver node and camera node. Subsequently, it commands the robot to identify color blocks based on voice-issued commands.

**1. Initiate launch File**

The launch file is saved in

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/launch/voice_control_color_detect.launch**

**(1) Initiate Several launch Files**

The **\<include\>** tag here is used to include other launch files. When this file is launched, other launch files will also be started simultaneously. Mainly, it initializes the microphone, drives the chassis control, and starts the color recognition files.

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image54.png" style="width:500px" />

**(2) Start the Node**

In the launch file,`<node>` represents the node being initiated, with subsequent tags like `<name>`, `<pkg>`, `<type>`, `<output>`, etc., serving as parameters for the node.

`<name>` denotes the node's identifier.

`<pkg>` indicates the function package where the node is located.

`<type>` specifies the file executed by the node, such as "**voice_control_color_detect.py**" and "**init_pose.py**".

`<output>` designates the target for output, typically the screen.

`<param>` is a parameter tag stored in the parameter server. It contains the declared parameter, with `<default>` specifically defining the parameter value.

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image55.png" style="width:500px" />

**2. Python Program**

The source code of the program is located in

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/scripts/voice_control_color_detect.py**

```py
class VoiceControlColorDetectNode:
    def __init__(self, name):
        rospy.init_node(name)
        self.count = 0
        self.color = None
        self.running = True
        self.last_color = None
        signal.signal(signal.SIGINT, self.shutdown)

        self.language = os.environ['ASR_LANGUAGE']
        rospy.Subscriber('/asr_node/voice_words', String, self.words_callback)
        rospy.Subscriber('/color_detect/color_info', ColorsInfo, self.get_color_callback)
        
        camera = rospy.get_param('/depth_camera/camera_name', 'depth_cam')  # 获取参数
        rospy.wait_for_message('/%s/rgb/image_raw' % camera, Image)
        rospy.wait_for_service('/voice_control/get_offline_result')

        self.play('running')
        self.buzzer_pub = rospy.Publisher('/ros_robot_controller/set_buzzer', BuzzerState, queue_size=1)
        rospy.loginfo('唤醒口令: 小幻小幻(Wake up word: hello hiwonder)')
        rospy.loginfo('唤醒后15秒内可以不用再唤醒(No need to wake up within 15 seconds after waking up)')
        rospy.loginfo('控制指令: 开启颜色识别 关闭颜色识别(Voice command: start color recognition/stop color recognition)')

        while self.running:
            if self.color == 'red' and self.last_color != 'red':
                self.last_color = 'red'
                self.play('red')
                print('red')
            elif self.color == 'green' and self.last_color != 'green':
                self.last_color = 'green'
                self.play('green')
                print('green')
            elif self.color == 'blue' and self.last_color != 'blue':
                self.last_color = 'blue'
                self.play('blue')
                print('blue')
            else:
                self.count += 1
                rospy.sleep(0.01)
                if self.count > 50:
                    self.count = 0
                    self.last_color = self.color
        rospy.signal_shutdown('shutdown')
```

**(1) Subscribe to Service**

Subscribe to the color recognition nodes to receive updates.

```py
        rospy.Subscriber('/asr_node/voice_words', String, self.words_callback)
        rospy.Subscriber('/color_detect/color_info', ColorsInfo, self.get_color_callback)
        
        camera = rospy.get_param('/depth_camera/camera_name', 'depth_cam')  # 获取参数
        rospy.wait_for_message('/%s/rgb/image_raw' % camera, Image)
        rospy.wait_for_service('/voice_control/get_offline_result')

        self.play('running')
        self.buzzer_pub = rospy.Publisher('/ros_robot_controller/set_buzzer', BuzzerState, queue_size=1)
        rospy.loginfo('唤醒口令: 小幻小幻(Wake up word: hello hiwonder)')
        rospy.loginfo('唤醒后15秒内可以不用再唤醒(No need to wake up within 15 seconds after waking up)')
        rospy.loginfo('控制指令: 开启颜色识别 关闭颜色识别(Voice command: start color recognition/stop color recognition)')
```

**(2) Set wake Command & Voice-Control Command**

```py
        rospy.Subscriber('/asr_node/voice_words', String, self.words_callback)
        rospy.Subscriber('/color_detect/color_info', ColorsInfo, self.get_color_callback)
        
        camera = rospy.get_param('/depth_camera/camera_name', 'depth_cam')  # 获取参数
        rospy.wait_for_message('/%s/rgb/image_raw' % camera, Image)
        rospy.wait_for_service('/voice_control/get_offline_result')

        self.play('running')
        self.buzzer_pub = rospy.Publisher('/ros_robot_controller/set_buzzer', BuzzerState, queue_size=1)
        rospy.loginfo('唤醒口令: 小幻小幻(Wake up word: hello hiwonder)')
        rospy.loginfo('唤醒后15秒内可以不用再唤醒(No need to wake up within 15 seconds after waking up)')
        rospy.loginfo('控制指令: 开启颜色识别 关闭颜色识别(Voice command: start color recognition/stop color recognition)')
```

**(3) Wake Up Robot Car**

Upon successful matching of the wake word, play the corresponding voice using the open_success module.

```
    def words_callback(self, msg):
        words = json.dumps(msg.data, ensure_ascii=False)[1:-1]
        if self.language == 'Chinese':
            words = words.replace(' ', '')
        print('words:', words)
        if words is not None and words not in ['唤醒成功(wake-up-success)', '休眠(Sleep)', '失败5次(Fail-5-times)',
                                               '失败10次(Fail-10-times']:
            if words == '开启颜色识别' or words == 'start color recognition':
```

**(4) Voice Control**

Match the recognized sentences, play the corresponding voice using the **voice_play** module, then set the corresponding color based on the recognized content, and play the corresponding voice.

```
        while self.running:
            if self.color == 'red' and self.last_color != 'red':
                self.last_color = 'red'
                self.play('red')
                print('red')
            elif self.color == 'green' and self.last_color != 'green':
                self.last_color = 'green'
                self.play('green')
                print('green')
            elif self.color == 'blue' and self.last_color != 'blue':
                self.last_color = 'blue'
                self.play('blue')
                print('blue')
            else:
                self.count += 1
                rospy.sleep(0.01)
                if self.count > 50:
                    self.count = 0
                    self.last_color = self.color
        rospy.signal_shutdown('shutdown')
```

### 13.3.4 Voice-Controlled Color Tracking

This lesson focuses on controlling the robot to track specific colors using voice commands. It also covers the principles of voice control and color tracking.

* **Overview**

First, subscribe to the voice recognition service published by the microphone array node. The system processes the incoming voice data through localization, noise reduction, and recognition, and retrieves both the recognized sentence and the angle of the sound source.

Next, use the designated wake word to activate the robot. Once the wake word is detected, the robot provides corresponding voice feedback and response.

Then, publish the corresponding command. The robot tracks the target color according to the received instruction.

The implementation of target tracking consists of two parts: color recognition and tracking.

In the color recognition stage, apply Gaussian filtering to reduce image noise. Convert the object’s color to the Lab color space. (For more information about the Lab color space, refer to **[9 ROS1-ROS+OpenCV Lesson]()**.)

Based on the Lab conversion, use color thresholds to identify the object’s color within the specified region. Apply a mask to the image to selectively process specific areas while blocking others.

Then perform morphological operations—specifically opening and closing—to refine the recognition results.

The tracking stage uses a PID algorithm, which compares the target’s pixel coordinates with the frame’s center coordinates and continuously minimizes the positional error to achieve stable tracking.

The PID algorithm is one of the most widely used automatic control methods. It adjusts the control output according to the proportional, integral, and derivative of the error. Due to its simplicity, ease of implementation, broad applicability, and independently adjustable parameters, the PID controller remains a common and effective control strategy.

* **Preparation**

1) Please ensure the microphone array, sound card and speaker are connected to the corresponding port of USB hub.

2) Please refer to the "[**13.3.1 Must-Read Six-Microphone Array Installation**]()" section in this catalogue to complete the application of APPID and the replacement of files for the Jetson Nano part.

3) The default system wake word is "hello hiwonder" in English. To switch to Chinese, refer to the documentation under the directory of **[13.4 Switching Wake Words Between Chinese and English]()**

* **Operation Steps**

> [!NOTE]
>
> **Notice:**
>
> * **The input command should be case sensitive, and keywords can be complemented using Tab key.**
>
> * **When identifying color blocks, ensure that objects with colors similar or identical to the color blocks in the background are avoided to prevent interference.**
>
> * **If color recognition is inaccurate, you can adjust the color threshold by referring to "[9 ROS1-ROS+OpenCV Lesson -> 9.1 Color Threshold Adjustment]()".**

1. Start the robot, and access the robot system desktop using remote control software NoMachine. For connection instructions, please refer to: **[1 Quick Start Guide]()**

2. Click-on <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image25.png" style="width:50px" /> to open the command-line terminal.

3. Run the command and hit Enter to disable the app auto-start service.

   ```py
   sudo systemctl stop start_app_node.service
   ```

4. Execute the command and hit Enter to start the game.

   ```py
   roslaunch xf_mic_asr_offline voice_control_color_track.launch
   ```

5. If you want to terminate the program, use short-cut ‘**Ctrl+C**’. If the program fails to stop, please retry.

6. To access the live camera feed, open a new terminal and execute the command '**rqt_image_view**' to activate the visualization tool. Then, select the color recognition and sorting channel '**/color_detect/image_result**'.

   <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image53.png" style="width:500px" />

7. To revisit the game of the mobile app later, simply enter command to restart the related services, press Enter to execute the command, and wait for the robot arm to return to its initial posture.

   ```py
   sudo systemctl start start_app_node.service
   ```

* **Program Outcome** 

> [!NOTE]
>
> **Notice:**
>
> * **For optimal performance, ensure you are in a relatively quiet environment.**
>
> * **Before issuing each voice command, start by saying the wake word.**
>
> * **Speak loudly and clearly when giving voice commands.**
>
> * **Give voice commands one at a time and wait for the robot to provide feedback before issuing additional commands.**

<span class="mark">To initiate game, begin by stating the wake word "**Hello Hiwonder**," followed by the command "**Start color tracking**" to prompt the robot to start recognizing colors. The recognizable colors for this activity are red, green, and blue. Upon recognizing a color, such as red, position a red square within the camera's field of view. Once the color block is identified, the robot will announce the color name, such as "**red**."</span>

<span class="mark">If you wish to cease color recognition, please state the wake word "**Hello Hiwonder**" followed by the command "**Stop tracking**".</span>

* **Program Analysis**

<span class="mark">Voice-controlled color tracking enables the voice control node to establish communication with the camera node. Subsequently, it commands the robot to identify and track color blocks based on voice-issued commands.</span>

**1. Initiate launch File**

The launch file is located in:

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/launch/voice_control_color_track.launch**

**(1) Initiate Several Launch Files**

The **\<include\>** tag is utilized here to incorporate other launch files. Upon starting this file, other launch files will also be initiated simultaneously. Mainly, we initialize the microphone and start the color tracking startup file.

```py
    <include file="$(find xf_mic_asr_offline)/launch/mic_init.launch"/>
    <include file="$(find hiwonder_example)/scripts/color_track/color_track_node.launch">
        <arg name="start"           value="false"/>
    </include>
```

**(2) Activate Node**

In this file, `<node>` represents the node initiated, and subsequent tags such as `<name>`, `<pkg>`, `<type>`, and `<output>` are parameters for the node.

`<name>` specifies the node's identifier.

`<pkg>` denotes the function package where the node is located.

`<type>` indicates the name of the file executed by the node, which in this case is the content of the "**voice_control_color_track.py**" file.

`<output>` designates the target for output, typically the screen.

```py
<node pkg="xf_mic_asr_offline" type="voice_control_color_track.py" name="voice_control_color_track_node" output="screen"/>
```

**2. Python Program**

**(1) Subscribe to Service**

The program is saved in

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/scripts/voice_control_color_track.py**

**(2) Awake Robot**

Upon successful matching of the wake word, the corresponding voice is played using the voice_play module.

```py
    def words_callback(self, msg):
        words = json.dumps(msg.data, ensure_ascii=False)[1:-1]
        if self.language == 'Chinese':
            words = words.replace(' ', '')
        print('words:', words)
        if words is not None and words not in ['唤醒成功(wake-up-success)', '休眠(Sleep)', '失败5次(Fail-5-times)',
                                               '失败10次(Fail-10-times']:
            if words == '追踪红色' or words == 'track red object':
                res = rospy.ServiceProxy('/color_track/set_color', SetString)("red")
                if res.success:
                    self.play('start_track_red')
                else:
                    self.play('track_fail')
            elif words == '追踪绿色' or words == 'track green object':
                res = rospy.ServiceProxy('/color_track/set_color', SetString)("green")
                if res.success:
                    self.play('start_track_green')
                else:
                    self.play('track_fail')
            elif words == '追踪蓝色' or words == 'track blue object':
                res = rospy.ServiceProxy('/color_track/set_color', SetString)("blue")
                if res.success:
                    self.play('start_track_blue')
                else:
                    self.play('track_fail')
            elif words == '停止追踪' or words == 'stop tracking':
                res = rospy.ServiceProxy('/color_track/set_color', SetString)()
                if res.success:
                    self.play('stop_track')
                else:
                    self.play('stop_fail')
        elif words == '唤醒成功(wake-up-success)':
            self.play('awake')
        elif words == '休眠(Sleep)':
            msg = BuzzerState()
            msg.freq = 1900
            msg.on_time = 0.05
            msg.off_time = 0.01
            msg.repeat = 1
            self.buzzer_pub.publish(msg)
```

### 13.3.5 Voice-Controlled Color Sorting

This lesson focuses on initiating the color block sorting game using voice control of the robot. Once the color blocks are identified, they will be placed in their corresponding positions. Additionally, the underlying principles will be introduced.

* **Overview**

1)  First, the system subscribes to the voice recognition service published by the microphone array node. It processes the incoming voice data through localization, noise reduction, and recognition, and then retrieves the recognized sentence and the angle of the sound source.

Next, after successfully waking up the robot and speaking specific commands, the robot will provide corresponding voice feedback responses.

Finally, publish the corresponding command. The robot will enable or disable color sorting based on the command.

2)  Color sorting consists of three main steps: ROI area division, color recognition, and color sorting.

The first step is ROI area division, where you select a Region of Interest (ROI) within the camera's field of view. This ROI will undergo color recognition, represented by the yellow box visible on the return screen after initiating game.

ROI, or Region of Interest, refers to the specific area in the image targeted for processing. It's typically outlined as boxes, circles, or ellipses in image processing applications.In computer vision and image processing, an ROI refers to a specific area of an image, such as a rectangle, circle, ellipse, or polygon, that is selected for further analysis or processing.

For color recognition, begin by applying Gaussian filtering to reduce image noise. Then, convert the item's color using the Lab color space.

Based on this, color thresholds are used to identify the color of the object within the circle. A mask is then applied to the image, which involves selecting parts of the image, graphics, or objects to globally or locally block out areas in the image for processing.

After masking, morphological operations including opening and closing are performed on the object image to refine the results.

3)  When recognizing a specific color, the robot arm will lower to the designated position to pick up the block and then place it at a specific location.

* **Preparation**

1\) Please ensure the microphone array, sound card and speaker are connected to the corresponding port of USB hub.

2\) Please refer to the "[**13.3.1 Must-Read Six-Microphone Array Installation**]()" section in this catalogue to complete the application of APPID and the replacement of files for the Jetson Nano part.

3\) The default system wake word is "hello hiwonder" in English. To switch to Chinese, refer to the documentation under the directory of **[13.4 Switching Wake Words Between Chinese and English]()**

* **Operation Steps**

> [!NOTE]
>
> **Notice:**
>
> * **The input command should be case sensitive, and keywords can be complemented using Tab key.**
>
> * **When identifying color blocks, ensure that objects with colors similar or identical to the color blocks in the background are avoided to prevent interference.**
>
> * **If color recognition is inaccurate, you can adjust the color threshold by referring to "[9 ROS1-ROS+OpenCV Lesson -> 9.1 Color Threshold Adjustment]()".**

1. Power on the robot and connect it via the NoMachine remote control software. Refer to: **[1. Quick Start Guide -> 1.6Development Environment Setup]()**

2. Click-on <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image25.png" style="width:50px" /> to open the command-line terminal.

3. Run the command and hit Enter to disable the app auto-start service.

   ```py
   sudo systemctl stop start_app_node.service
   ```

4. Run the command to initiate the game.

   ```py
   roslaunch xf_mic_asr_offline voice_control_color_sorting.launch
   ```

5. After saying the wake word, you can issue instructions to start color sorting.

6. If you need to terminate the program, use short-cut **“Ctrl+C”**. If the program cannot be stopped, please retry.

7. To revisit the game of the mobile app later, simply enter command to restart the related services, press **Enter** to execute the command, and wait for the robot arm to return to its initial posture.

   ```py
   sudo systemctl start start_app_node.service
   ```

* **Program Outcome**

Upon starting the game, first say the wake word and then issue the command to initiate color block sorting.

For instance, if you say the wake word "**Hello Hiwonder**" followed by the command "**Start color sorting**" the terminal will access the return screen and commence identifying color blocks within a specific area. Once the corresponding color block is recognized, it will be grasped and positioned accordingly.

* **Program Analysis**

Voice-controlled color sorting involves establishing a connection between the voice control node and the camera node. Subsequently, it enables control over the robot's execution of game by issuing commands through voice.

**1. launch File**

The launch file is saved in

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/launch/voice_control_color_track.launch**

**(1) Initiate Several Launch Files**

The **`<include>`** tag is utilized here to incorporate other launch files. Upon starting this file, other launch files will also be initiated simultaneously. Mainly, we initialize the microphone and start the color sorting launch file.

```
    <include file="$(find xf_mic_asr_offline)/launch/mic_init.launch"/>
    <include file="$(find hiwonder_example)/scripts/color_track/color_track_node.launch">
        <arg name="start"           value="false"/>
    </include>
```

**(2) Activate Node**

In this file, `<node>` represents the node initiated, with subsequent parameters such as `<name>`, `<pkg>`, `<type>`, and `<output>`.

`<name>` specifies the node's identifier.

`<pkg>` denotes the function package where the node is located.

`<type>` indicates the name of the file executed by the node, which in this case is the content of the "**voice_control_color_sorting.py**" file.

`<output>` designates the target for output, typically the screen.

```
    <node pkg="xf_mic_asr_offline" type="voice_control_color_track.py" name="voice_control_color_track_node" output="screen"/>
```

**2. Python Program**

**(1) Subscribe to Service**

The program is saved in

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/scripts/voice_control_sorting.py**

Subscribe to the speech recognition node "**/voice_words**" to receive recognized speech information.

```
rospy.Subscriber('/asr_node/voice_words', String, self.words_callback)
```

**(2) Voice Wake Up Robot**

```
rospy.loginfo('唤醒口令: 小幻小幻(Wake up word: hello hiwonder)')
```

**(3) Match voice commands to enable or disable color block sorting on the robot**

```
            if words == '开启颜色分拣' or words == 'start color sorting':
                res = rospy.ServiceProxy('/color_sorting/start', Trigger)()
                if res.success:
                    self.play('open_success')
                else:
                    self.play('open_fail')
            elif words == '关闭颜色分拣' or words == 'stop color sorting':
                res = rospy.ServiceProxy('/color_sorting/stop', Trigger)()
                if res.success:
                    self.play('close_success')
                else:
                    self.play('close_fail')
```

### 13.3.6 Voice-Controlled Waste Sorting

<span class="mark">This lesson covers using voice-controlled robots to identify and sort waste cards.</span>

* **Overview**

First, the system subscribes to the voice recognition service published by the microphone array node. It processes the incoming voice data through localization, noise reduction, and recognition, and then retrieves the recognized sentence and the angle of the sound source.

Next, after successfully waking up the robot and speaking specific commands, the robot will provide corresponding voice feedback responses.

Finally, perform sentence matching. Based on the matching result, the robot will execute the corresponding action.

* **Preparation**

1) Please ensure the microphone array, sound card and speaker are connected to the corresponding port of USB hub.

2) Please refer to the "[**13.3.1 Must-Read Six-Microphone Array Installation**]()" section in this catalogue to complete the application of APPID and the replacement of files for the Jetson Nano part.

3) The default system wake word is "hello hiwonder" in English. To switch to Chinese, refer to the documentation under the directory of **[13.4 Switching Wake Words Between Chinese and English]()**

* **Operation Steps**

> [!NOTE]
>
> **Note: The input command should be case sensitive, and keywords can be complemented using Tab key.**

1. Power on the robot and connect it via the NoMachine remote control software. Refer to: **[1. Quick Start Guide ->1.6 Development Environment Setup]()**

2. Click-on <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image25.png" style="width:50px" /> to open the command-line terminal.

3. Enter the following command and press Enter to stop the APP service:

   ```py
   sudo systemctl stop start_app_node.service
   ```

4. Execute the command to start the feature

   ```py
   roslaunch xf_mic_asr_offline voice_control_garbage_classification.launch
   ```

5. If you need to terminate this game, use short-cut ‘**Ctrl+C**’. If the program fails to stop, please retry.

6. If you wish to re-experience the mobile app features later, you need to enter the command to restart the relevant service. After entering and executing the command, wait for the robotic arm to return to its initial posture.

   ```py
   sudo systemctl start start_app_node.service
   ```

* **Program Outcome**

Upon starting the game, begin by saying the wake word "**Hello, Hiwonder**" followed by the command to control the robot to start or stop waste sorting.

This game involves identifying four categories of waste: kitchen waste, hazardous waste, other waste, and recyclable waste. Let's take kitchen waste as an example. Start with the wake word "**Hello, Hiwonder**" and then issue the command "**Start waste sorting**". Subsequently, when a waste card is identified, the robotic arm will descend to pick up the card and place it into the corresponding junk area.

Here are the different waste cards and their corresponding placement areas (from the robot's perspective):

|   **Category**   |             **Placement area**             |
| :--------------: | :----------------------------------------: |
|    Food waste    | Positioned on the left front of the robot  |
| Hazardous waste  |  Positioned on the left side of the robot  |
|   Other waste    | Positioned on the right side of the robot  |
| Recyclable waste | Positioned on the right front of the robot |

* **Program Analysis**

**1. launch File**

Voice-controlled waste sorting enables the voice-controlled node to establish communication with the camera node. It allows the robot to toggle the game on or off by receiving voice commands and subsequently report its classification after picking up the corresponding garbage card.

The launch file path is located at:

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/launch/voice_control_garbage_classification.launch**

**(1) Activate Several launch Files**

The \<include\> tag is utilized here to incorporate other launch files. When this file is initiated, it will concurrently start other launch files. Primarily, we initialize the microphone and launch the garbage sorting startup file.

```
    <include file="$(find xf_mic_asr_offline)/launch/mic_init.launch"/>

    <include file="$(find hiwonder_example)/scripts/garbage_classification/garbage_classification.launch">
        <arg name="start"       value="false"/>
        <arg name="broadcast"   value="true"/>
    </include>
```

**(2) Initiate Node**

In this context, `<node>` represents the node initialized in the file, with subsequent parameters such as `<name>`, `<pkg>`, `<type>`, and `<output>`.

`<name>` denotes the identifier of the node.

`<pkg>` specifies the function package where the node is located.

`<type>` indicates the name of the file executed by this node, which in this case is the content of the "**voice_control_color_sorting.py**" file.

`<output>` designates the target for output, typically the screen. 

```
   <node pkg="xf_mic_asr_offline" type="voice_control_garbage_classification.py" name="voice_control_garbage_classification_node" output="screen"/>
```

* **Python Program**

**(1) Subscribe to Service**

Subscribe to the service for obtaining speech recognition words and awakening angles to retrieve the recognized speech information.

```
rospy.Subscriber('/asr_node/voice_words', String, self.words_callback)
```

**(2) Wake Up Robot Car**

Upon successful matching of the wake word, the corresponding voice feedback is played through the voice_play module.

```
    def play(self, name):
        voice_play.play(name, language=self.language)
```

**(3) Execute the corresponding action based on the voice command**

```
            if words == '开启颜色分拣' or words == 'start color sorting':
                res = rospy.ServiceProxy('/color_sorting/start', Trigger)()
                if res.success:
                    self.play('open_success')
                else:
                    self.play('open_fail')
            elif words == '关闭颜色分拣' or words == 'stop color sorting':
                res = rospy.ServiceProxy('/color_sorting/stop', Trigger)()
                if res.success:
                    self.play('close_success')
                else:
                    self.play('close_fail')
```

### 13.3.7 Voice-Controlled Multi-Point Navigation

This lesson involves utilizing voice-controlled robots to navigate on an existing map.

* **Overview**

First, launch the robot navigation service and load the map, then start the multi-point navigation service.

Next, subscribe to the voice recognition service published by the microphone array node. After performing sound source localization, noise reduction, and speech recognition, obtain the recognized text and the sound source angle.

Then, process the speech input through the microphone. When the wake word and control commands are recognized and meet the set recognition threshold, the robot will provide corresponding voice feedback.

Finally, based on the recognized commands, the robot will navigate to the corresponding location on the map. During navigation, global path planning is performed first, and if obstacles are encountered during movement, local path planning is activated.

* **Preparation**

1)  Before beginning, ensure that the voice module is properly installed on the robot, and connected to the USB ports of the USB hub.

2)  Please refer to the "[**13.3.1 Must-Read Six-Microphone Array Installation**]()" section in this catalogue to complete the application of APPID and the replacement of files for the Jetson Nano part.

3)  The default system wake word is "hello hiwonder" in English. To switch to Chinese, refer to the documentation under the directory of **[13.4 Switching Wake Words Between Chinese and English]()**

4)  Build a map of the area where the robot is currently located. To learn how to perform mapping, please refer to the documents in the directory: **[6. ROS1-Mapping Navigation Lesson]().**

5)  Place the robot on an open platform, ensuring there is sufficient activity space within a 3-meter radius centered around the robot.

* **Operation Steps**

> [!NOTE]
>
> **Notice: When entering commands, be sure to use correct case and spacing. You can use the Tab key to auto-complete keywords.**

1. Power on the robot and connect it via the NoMachine remote control software. Refer to: **[1. Quick Start Guide -> 1.6 Development Environment Setup And Configuration.]()**

2. Click the terminal icon <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image25.png" style="width:50px" /> in the system desktop to open a command-line window.

3. Enter the following command and press Enter to stop the APP service:

   ```py
   sudo systemctl stop start_app_node.service
   ```

4. Execute the command to start the feature.

   ```py
   roslaunch xf_mic_asr_offline voice_control_navigation.launch map:=map_01
   ```

The "map_01" at the end of the command is the map name. Users can modify this parameter according to their needs. The map storage path is **/home/hiwonder/ros_ws/src/hiwonder_slam/maps.**

5. To exit the feature, press **Ctrl+C** in the terminal. If the program does not close successfully, try pressing Ctrl+C again.

6. If you wish to re-experience the mobile app features later, you need to enter the command to restart the relevant service. After entering and executing the command, wait for the robotic arm to return to its initial posture.

   ```py
   sudo systemctl start start_app_node.service
   ```

* **Function Outcome**

Say **"hello hiwonder"** followed by a command phrase to control the robot's movement.

For example, first say "hello hiwonder". The robot will respond with **"I'm here"**. Then you can say **"go to point A"**, and the robot will proceed to the upper-right area relative to the starting position.

The command phrases and their corresponding functions are listed in the table below (using the robot's first-person perspective):

| **Voice commandCommand** |                         **Function**                         |
| :----------------------: | :----------------------------------------------------------: |
|      Go to A point       | Control the robot to move to point A: Upper right of the starting position |
|      Go to B point       | Control the robot to move to point B: Upper left of the starting position |
|       Go to Cpoint       |     Control the robot to move to point C: Below point A      |
|   Go back to the start   | Control the robot to move to the origin point: Starting position |

* **Program Analysis**

**1. Launch File**

The launch file path is located at:

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/launch/voice_control_navigation.launch**

**(1) Initiate Several launch Files**

The `<include>` tag is utilized to incorporate other launch files. When this file is launched, it will simultaneously start all other launch files.

This primarily encompasses the initialization of the microphone, navigation, and simulation software startup files.

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image79.png" style="width:800px" />

**(2) Initiate Node**

The `<node>` tag represents the node initiated within the file, with subsequent parameters such as \<name\>, `<pkg>`, `<type>`, `<output>`, etc.

`<name>` signifies the node's identifier. 
`<pkg>` denotes the package containing the node's functionality.

`<type>` specifies the name of the file executed by the node. Here, it executes the content of the "**voice_control_color_sorting.py**" file.

`<output>` denotes the sending target, typically the screen.

`<ns>` is a parameter enabling the execution of nodes in a custom space named by the user. 

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image80.png" style="width:800px" />

**2. Python Program**

**(1) Wake Up Robot Car**

Upon successful matching of the wake word, the corresponding voice feedback is played through the voice_play module

```py
    def words_callback(self, msg):
        words = json.dumps(msg.data, ensure_ascii=False)[1:-1]
        if self.language == 'Chinese':
            words = words.replace(' ', '')
        print('words:', words)
        if words is not None and words not in ['唤醒成功(wake-up-success)', '休眠(Sleep)', '失败5次(Fail-5-times)',
                                               '失败10次(Fail-10-times']:
            if words == '开启颜色分拣' or words == 'start color sorting':
                res = rospy.ServiceProxy('/color_sorting/start', Trigger)()
                if res.success:
                    self.play('open_success')
                else:
                    self.play('open_fail')
            elif words == '关闭颜色分拣' or words == 'stop color sorting':
                res = rospy.ServiceProxy('/color_sorting/stop', Trigger)()
                if res.success:
                    self.play('close_success')
```

**(2) Print Sound Source Angle**

Determine the robot's turning angle based on the sound source angle recognized by the microphone array.

```py
    def angle_callback(self, msg):
        self.angle = msg.data
        print('angle:', self.angle)
```

**(3) Voice Control**

Match the recognized text and play the corresponding voice through the voice_play module. Then, based on the recognized content, set the corresponding linear or angular velocity. This allows the underlying driver node mecanum_pub to publish messages and control the robot's movement.

```
        while not rospy.is_shutdown() and self.running:
            if self.words is not None:
                pose = PoseStamped()
                pose.header.frame_id = self.map_frame
                pose.header.stamp = rospy.Time.now()
                if self.words == '去\'A\'点' or self.words == 'go to A point':
                    print('>>>>>> go a')
                    pose.pose.position.x = 1
                    pose.pose.position.y = -1
                    pose.pose.orientation.w = 1
                    self.play('go_a')
                    self.goal_pub.publish(pose)
                elif self.words == '去\'B\'点' or self.words == 'go to B point':
                    print('>>>>>> go b')
                    pose.pose.position.x = 2
                    pose.pose.position.y = 0
                    pose.pose.orientation.w = 1
                    self.play('go_b')
                    self.goal_pub.publish(pose)
                elif self.words == '去\'C\'点' or self.words == 'go to C point':
                    print('>>>>>> go c')
                    pose.pose.position.x = 1
                    pose.pose.position.y = 1
                    pose.pose.orientation.w = 1
                    self.play('go_c')
                    self.goal_pub.publish(pose)
                elif self.words == '回原点' or self.words == 'go back to the start':
                    print('>>>>>> go origin')
                    pose.pose.position.x = 0
                    pose.pose.position.y = 0
                    pose.pose.orientation.w = 1
                    self.play('go_origin')
                    self.goal_pub.publish(pose)
```

* **Function Expansion** 

The default point A in the program corresponds to the upper right corner of the robot's starting position map, with coordinates (1, -1) in meters. To change the position of point A, follow these steps, using the example of relocating point A to the lower right of the starting position:

> [!NOTE]
>
> **Note:**
>
> * **Users can refer to this section to adjust the positions of point B and point C as needed.**
>
> * **Please ensure strict case sensitivity when entering commands, and feel free to use the "Tab" key for auto-completion of keywords.**

1. Start the robot and connect it to the remote control software NoMachine. For connection instructions, please refer to: **[1. Quick Start Guide -> 6.Development Environment Setup]()**

2. Click the terminal icon <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image25.png" style="width:50px" /> in the system desktop to open a command-line window.

3. Enter the following command and press Enter to go to the directory containing the programs

   ```py
   roscd xf_mic_asr_offline/scripts/
   ```

4. Open the program file by entering the command.

   ```py
   vim voice_control_navigation.py
   ```

5)  Locate the following code.

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image87.png" style="width:500px" />

6)  Press i to enter edit mode, Change ‘1’ to ‘-1’.

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image88.png" style="width:500px" />

From the robot's perspective, the positive X-axis direction is forward, and the positive Y-axis direction is leftward. Hence, adjusting the X-axis coordinate of point A from positive to negative will relocate the point to the lower right side of the robot.

7)  After editing, press “**Esc**”, then type :wq to save and close the file.

<img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image89.png" style="width:500px" />

8)  Refer to "[**13.3.7 Voice-Controlled Multi-Point Navigation->Operation Steps**]()" and restart the application to observe the modified gameplay effects.

### 13.3.8 Voice-Controlled Navigation transportation

This lesson involves utilizing voice-controlled robots to navigate and transport on an existing map.

* **Overview**

First, launch the robot navigation service and load the map, then start the multi-point navigation service.

Next, subscribe to the voice recognition service published by the microphone array node. After performing sound source localization, noise reduction, and speech recognition, obtain the recognized text and the sound source angle.

Then, process the speech input through the microphone. When the wake word and control commands are recognized and meet the set recognition threshold, the robot will provide corresponding voice reply.

Finally, based on the recognized commands, the robot will navigate to the corresponding location on the map. During navigation, global path planning is performed first, and if obstacles are encountered during movement, local path planning is activated. The alignment and gripping service is called upon reaching the first point, and the placement service is called upon reaching the second point.

* **Preparation**

1)  Before beginning, ensure that the voice module is properly installed on the robot, and connected to the USB ports of the USB hub.

2)  Please refer to the "[**13.3.1 Must-Read Six-Microphone Array Installation**]()" section in this catalogue to complete the application of APPID and the replacement of files for the Jetson Nano part.

3)  The default system wake word is "hello hiwonder" in English. To switch to Chinese, refer to the documentation under the directory of **[13.4 Switching Wake Words Between Chinese and English]()**

4)  Build a map of the area where the robot is currently located. To learn how to perform mapping, please refer to the documents in the directory: **[6. ROS1- Mapping Navigation Lesson]().**

5)  Place the robot on an open platform, ensuring there is sufficient activity space within a 3-meter radius centered around the robot.

* **Operation Steps**

> [!NOTE]
>
> **Note: When entering commands, be sure to use correct case and spacing. You can use the Tab key to auto-complete keywords.**

1. Power on the robot and connect it via the NoMachine remote control software. Refer to: **[1. Quick Start Guide -> 1.6 Development Environment Setup]()**

2. Click the terminal icon <img src="../_static/media/3/section_125_2 Voice Interaction Application/media/image25.png" style="width:50px" /> in the system desktop to open a command-line window.

3. Enter the following command and press Enter to stop the APP service:

   ```py
   sudo systemctl stop start_app_node.service
   ```

4. Enter the command and press Enter to launch the feature.

   ```py
   roslaunch xf_mic_asr_offline voice_control_navigation_transport.launch map:=map_01
   ```

The "map_01" at the end of the command is the map name. Users can modify this parameter according to their needs. The map storage path is **/home/hiwonder/ros_ws/src/hiwonder_slam/maps.**

8. To exit the feature, press Ctrl+C in the terminal. If the program does not close successfully, try pressing Ctrl+C again.

9. If you wish to re-experience the mobile app features later, you need to enter the command to restart the relevant service. After entering and executing the command, wait for the robotic arm to return to its initial posture.

   ```py
   sudo systemctl start start_app_node.service
   ```

* **Function Outcome**

Say "**hello hiwonder**" followed by a command phrase to control the robot's movement.

For example, first say "**hello hiwonder**". The robot will respond with "**I'm here**". Then we can issue the command "**Navigate and Transport**". The robot will then proceed to the coordinates (0, 0.5, 0) on the map to pick up the object. After successfully gripping the object, it will move to the coordinates (1.5, 0, 0) to place it.

* **Program Analysis**

**1. Launch File**

```py
<?xml version="1.0"?>
<launch>
    <!--
        pick(1.5, 0.5)  place(1.0, 1.0)
            |               |
            |               |
    map——————————————————————
    -->
    <arg name="map"                 default=""/>
    <arg name="place_without_color" default="true"/>
    <arg name="broadcast"           default="true"/>
    <arg name="pick_position"       default="[1.5, 0.5, 0, 0, 0]"/> <!--x, y, roll, pitch, yaw-->
    <arg name="place_position"      default="[1.0, 1.0, 0, 0, 0]"/> <!--x, y, roll, pitch, yaw-->

    <include file="$(find xf_mic_asr_offline)/launch/mic_init.launch"/>

    <include file="$(find hiwonder_example)/scripts/navigation_transport/navigation_transport.launch">
        <arg name="map"                 value="$(arg map)"/>
        <arg name="place_without_color" value="$(arg place_without_color)"/>
        <arg name="broadcast"           value="$(arg broadcast)"/>
    </include>

    <node pkg="xf_mic_asr_offline" type="voice_control_navigation_transport.py" name="voice_control_navigation_transport_node" output="screen">
        <rosparam param="pick_position"  subst_value="True">$(arg pick_position)</rosparam>
    </node>
</launch>
```

The launch file path is located at:

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/launch/voice_control_navigation_transport.launch**

**(1) Initiate Several launch Files**

The `<include>` tag is utilized to incorporate other launch files. When this file is launched, it will simultaneously start all other launch files.

This primarily encompasses the initialization of the microphone and navigation startup files. Control how the robot performs grasping and placement tasks based on preset positions by passing parameters.

```py
    <arg name="map"                 default=""/>
    <arg name="place_without_color" default="true"/>
    <arg name="broadcast"           default="true"/>
    <arg name="pick_position"       default="[1.5, 0.5, 0, 0, 0]"/> <!--x, y, roll, pitch, yaw-->
    <arg name="place_position"      default="[1.0, 1.0, 0, 0, 0]"/> <!--x, y, roll, pitch, yaw-->

    <include file="$(find xf_mic_asr_offline)/launch/mic_init.launch"/>

    <include file="$(find hiwonder_example)/scripts/navigation_transport/navigation_transport.launch">
        <arg name="map"                 value="$(arg map)"/>
        <arg name="place_without_color" value="$(arg place_without_color)"/>
        <arg name="broadcast"           value="$(arg broadcast)"/>
    </include>
```

**(2) Initiate Node**

The `<node>` tag represents the node initiated within the file, with subsequent parameters such as `<name>`, `<pkg>`, `<type>`,`<output>`, etc.

`<name>` signifies the node's identifier.  
`<pkg>` denotes the package containing the node's functionality.

`<type>` specifies the name of the file executed by the node. Here, it executes the content of the "**voice_control_color_sorting.py**" file.

`<output>` denotes the sending target, typically the screen.

`<ns>` is a parameter enabling the execution of nodes in a custom space named by the user. 

```
    <node pkg="xf_mic_asr_offline" type="voice_control_navigation_transport.py" name="voice_control_navigation_transport_node" output="screen">
            <rosparam param="pick_position"  subst_value="True">$(arg pick_position)</rosparam>
```

**2. Python Program**

The source code for this program is located at:

**/home/hiwonder/ros_ws/src/xf_mic_asr_offline/scripts/voice_control_navigation_transport.py**

```py
    def __init__(self, name):
        rospy.init_node(name)
        self.running = True

        self.language = os.environ['ASR_LANGUAGE']
        self.pick_position = rospy.get_param('~pick_position')
        rospy.Subscriber('/asr_node/voice_words', String, self.words_callback)
        rospy.wait_for_service('/voice_control/get_offline_result')
        rospy.wait_for_message("/move_base/local_costmap/costmap", OccupancyGrid)
        self.play('running')
        self.buzzer_pub = rospy.Publisher('/ros_robot_controller/set_buzzer', BuzzerState, queue_size=1)
        rospy.loginfo('唤醒口令: 小幻小幻(Wake up word: hello hiwonder)')
        rospy.loginfo('唤醒后15秒内可以不用再唤醒(No need to wake up within 15 seconds after waking up)')
        rospy.loginfo('控制指令: 导航搬运(Voice command: navigate and transport)')
        try:
            rospy.spin()
        except Exception as e:
            rospy.logerr(str(e))
            rospy.loginfo("Shutting down")
```

**1. Environment Variable Retrieval**

```py
self.language = os.environ['ASR_LANGUAGE']
```

Obtain the configuration items ASR_LANGUAGE and MIC_TYPE from the system environment variables, representing the speech recognition language and microphone type respectively.

**2. Subscriber Creation**

```py
rospy.Subscriber('/asr_node/voice_words', String, self.words_callback)
```

Listen to the **/asr_node/voice_words** topic. When a message is received, call the **`self.words_callback`** method for processing.

**3. Publisher Creation**

```py
self.buzzer_pub = rospy.Publisher('/ros_robot_controller/set_buzzer', BuzzerState, queue_size=1)
```

Publish to the **/ros_robot_controller/set_buzzer** topic with the message type BuzzState.

**4. Define callback method**

```py
    def words_callback(self, msg):
        words = json.dumps(msg.data, ensure_ascii=False)[1:-1]
        if self.language == 'Chinese':
            words = words.replace(' ', '')
        print('words:', words)
        if words is not None and words not in ['唤醒成功(wake-up-success)', '休眠(Sleep)', '失败5次(Fail-5-times)',
                                               '失败10次(Fail-10-times']:
            if words == '导航搬运' or words == 'navigate and transport':
                pose = Pose2D()
                pose.x = self.pick_position[0]
                pose.y = self.pick_position[1]
                pose.roll = self.pick_position[2]
                pose.pitch = self.pick_position[3]
                pose.yaw = self.pick_position[4]
                res = rospy.ServiceProxy('/navigation_transport/pick', SetPose2D)(pose)
                if res.success:
                    self.play('start_navigating')
                else:
                    self.play('open_fail')
        elif words == '唤醒成功(wake-up-success)':
            self.play('awake')
        elif words == '休眠(Sleep)':
            msg = BuzzerState()
            msg.freq = 1900
            msg.on_time = 0.05
            msg.off_time = 0.01
            msg.repeat = 1
            self.buzzer_pub.publish(msg)
```

It receives voice commands and executes corresponding actions to achieve navigation and object transportation functions.

## 13.4 Switching Wake Words Between Chinese and English

By default, the system uses the English wake word "**hello hiwonder**". If you want to switch to Chinese wake words or commands, follow the steps below。

<img src="../_static/media/3/section_126_3 Switching Wake Words Between Chinese and English/media/image2.png" style="width:500px"  />

<p style="text-align:center">Six-Microphone Array</p>

1. For the Six-Microphone Array, use the configuration tool on the desktop to set and save the recognition language. Double-click the **"Tool"** application on the system desktop<img src="../_static/media/3/section_126_3 Switching Wake Words Between Chinese and English/media/image3.png" style="width:50px" />.

2. Set the language to Chinese, then click Save \> Quit.

   <img src="../_static/media/3/section_126_3 Switching Wake Words Between Chinese and English/media/image4.png" style="width:500px" />

3. After restarting the robot, the wake word will be successfully switched.

   